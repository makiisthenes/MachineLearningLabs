{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb313cd3-04b2-48fb-840f-32d83118c681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "182e42f7-eb5b-4251-9735-2c2f58aa91f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7059, 0.6601, 1.0000]) a_j\n",
      "tensor([1.]) d_k\n",
      "update tensor([[[0.7059, 0.6601, 1.0000]]])\n",
      "output_layer tensor([[[ 0.7457,  0.3547, -1.5159]]])\n",
      "update tensor([[[ 0.0398, -0.3054, -2.5159]]])\n",
      "tensor([[[ 0.0398, -0.3054, -2.5159]]])\n"
     ]
    }
   ],
   "source": [
    "d_k = torch.tensor([1.])\n",
    "a_j =  torch.tensor([i.y for i  in model.hidden_layer] + [1])\n",
    "print(a_j, \"a_j\")\n",
    "print(d_k, \"d_k\")\n",
    "update = a_j * d_k.view(-1, 1,1)\n",
    "print(\"update\", update)\n",
    "\n",
    "output_layer = torch.stack(list(model.output_layer.parameters()))\n",
    "print(\"output_layer\", output_layer)\n",
    "\n",
    "updated_weights = output_layer - update\n",
    "print(\"update\", updated_weights)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for param, new_weight in zip(model.output_layer.parameters(), updated_weights):\n",
    "        param.copy_(new_weight)\n",
    "\n",
    "\n",
    "print(torch.stack(list(model.output_layer.parameters())))  # [[0., 0., 0.]] # 1,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "146a19fc-d761-415b-929f-f6f8db0d5af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_k review tensor([[[1.]],\n",
      "\n",
      "        [[2.]]])\n",
      "tensor([[[-0.0813, -0.1582,  0.5678]],\n",
      "\n",
      "        [[-0.1626, -0.3164,  1.1356]]])\n",
      "tensor([[-0.2439, -0.4746,  1.7034]])\n",
      "tensor([-0.2439, -0.4746])\n",
      "tensor([0.2134, 0.2496])\n",
      "tensor([-0.0520, -0.1185])\n"
     ]
    }
   ],
   "source": [
    "output_layer = torch.tensor([[[-0.0813, -0.1582,  0.5678]], [[-0.0813, -0.1582,  0.5678]]])\n",
    "d_k = torch.tensor([1.,2.])\n",
    "print(\"d_k review\", d_k.view(-1, 1,1))\n",
    "sum_d_j = output_layer * d_k.view(-1, 1,1)\n",
    "print(sum_d_j)\n",
    "sum_d_j = torch.sum(sum_d_j, dim=0)\n",
    "print(sum_d_j)\n",
    "print(sum_d_j[:, :-1][0])  # ignore the third value as this is the bias.\n",
    "d_j = [torch.tensor([0.2134]), torch.tensor([0.2496])]\n",
    "print(torch.cat(d_j))\n",
    "d_j = torch.cat(d_j) * sum_d_j[:, :-1][0]\n",
    "print(d_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "ae426b40-3a20-4835-ad94-b3cac7395a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.5781, -0.2402, -0.2009]],\n",
      "\n",
      "        [[-0.4334, -2.2371, -1.2284]]]) before hl_weights\n",
      "torch.Size([2, 1, 3]) torch.Size([2, 1, 3])\n",
      "tensor([[[ 0.5781, -0.2402, -0.1957]],\n",
      "\n",
      "        [[-0.4334, -2.2371, -1.2166]]]) after hl_weights\n"
     ]
    }
   ],
   "source": [
    "# weight update ij\n",
    "lr = 0.1\n",
    "x_test = torch.tensor([0., 0., 1.]).reshape(1,-1)\n",
    "d_j = torch.tensor([-0.0520, -0.1185]).reshape(1,-1)\n",
    "\n",
    "updated_weights = torch.matmul(d_j.T, x_test) * lr\n",
    "hl_weights = torch.stack(list(model.hidden_layer.parameters()))\n",
    "print(hl_weights, \"before hl_weights\")\n",
    "updated_weights = updated_weights.reshape(updated_weights.shape[0], 1,updated_weights.shape[1])\n",
    "print(updated_weights.shape, hl_weights.shape)\n",
    "# hidden_layer = torch.tensor([[[ 0.5429, -0.5430,  1.3089]], [[-0.1468, -0.7791, -0.5856]]])\n",
    "hl_weights = hl_weights - updated_weights\n",
    "print(hl_weights, \"after hl_weights\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for param, new_weight in zip(model.hidden_layer.parameters(), hl_weights):\n",
    "        param.copy_(new_weight)\n",
    "        \n",
    "\n",
    "# print(torch.stack(list(model.hidden_layer.parameters())))  # [[0., 0., 0.]] # 1,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27380c29-df9b-42ef-88a8-0d04f43cc00b",
   "metadata": {
    "id": "d3N_wzfmkK7h"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "  def __init__(self, num_inputs, num_neurons, num_outputs):\n",
    "    super().__init__()    \n",
    "    self.hidden_layer = nn.ModuleList([LogisticRegression(num_inputs) for i in range(num_neurons)])\n",
    "    self.output_layer = nn.ModuleList([LogisticRegression(num_neurons+1) for i in range(num_outputs)]) # +1 for the bias\n",
    "\n",
    "  def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "    y_hidden = torch.ones(len(self.hidden_layer) + 1) # +1 for the bias\n",
    "    y = torch.ones(len(self.output_layer))  \n",
    "    #print(f\"y hidden: \", y_hidden)\n",
    "    ### your code here\n",
    "    y_hidden = torch.tensor([layer(x) for layer in self.hidden_layer] + [1])\n",
    "    # for i, l in enumerate(self.hidden_layer):\n",
    "    #     y_hidden[i] = l(x)\n",
    "\n",
    "    #print(f\"y hidden: \", y_hidden)\n",
    "    for i, l in enumerate(self.output_layer):\n",
    "        y[i] = l(y_hidden)\n",
    "    #print(f\"y: {y}\")\n",
    "    return y\n",
    "\n",
    "  def backward_pass(self, x: torch.tensor, y: torch.tensor, lr: float) -> None:\n",
    "    ### your code here\n",
    "\n",
    "    #print(\"length of output layers\", len(self.output_layer))\n",
    "    y_pred = self.forward(x)\n",
    "    d_outputs = torch.zero(len(self.output_layer))\n",
    "    d_hidden = torch.ones(len(self.hidden_layer)+1)\n",
    "    \n",
    "    y_hidden = torch.tensor([layer(x) for layer in self.hidden_layer] + [1])\n",
    "    y_out = torch.tensor([layer(y_hidden) for layer in self.output_layer] + [1])\n",
    "\n",
    "    # error with respect to j.\n",
    "    # Equivalent to length of hidden neurons\n",
    "    (y_pred - y) \n",
    "\n",
    "    \n",
    "    for index in range(len(self.output_layer)):\n",
    "    # To do the backward, we need to understand the partial derivatives and the chain rule, for it to completely make sense.\n",
    "         d_outputs[index] = ((y_pred[index]-y[index]) * (y_pred[index]*(1-y_pred[index])))\n",
    "\n",
    "\n",
    "    # sum [[neuron1, neuron2, neuron3] torch.tensor(elem.weight) * d_hidden[index] for index, elem in enumerate(self.output_layer)]\n",
    "    hidden_sum = []\n",
    "    for index, elem in enumerate(self.output_layer):\n",
    "      hidden_sum.append( elem.weight * d_hidden[index])\n",
    "    hidden_sum = torch.sum(hidden_sum[0], dim=0)\n",
    "      \n",
    "    for index in range(len(d_hidden)):\n",
    "        # sum of all k outputs that specific nodex j goes to. Times the weight value j to k and the output k.\n",
    "        # once we have done this, the model will be able to achieve better accuracy. \n",
    "        d_hidden[index] = y_hidden[index] * (1-y_hidden[index]) * hidden_sum[index]\n",
    "\n",
    "\n",
    "    # Update weights of output layer. (dimension num_outputs , hidden+1)\n",
    "    for output_index in range(len(self.output_layer)):\n",
    "        for index, weight in enumerate(self.output_layer[output_index].weight):\n",
    "            # print(f\"New weight: { lr * d_outputs[output_index] * weight }\")\n",
    "            # print(lr, d_outputs[output_index], weight)\n",
    "            self.output_layer[output_index].weight[index] =  self.output_layer[output_index].weight[index] - (lr * d_outputs[output_index] * y_out[output_index])\n",
    "\n",
    "\n",
    "    hidden_weights = torch.stack(list(self.hidden_layer.parameters()))\n",
    "    hidden_weights = [inner[0].tolist() for inner in hidden_weights]\n",
    "    # print(\"self.hidden weights\", hidden_weights)\n",
    "    # [[0.000819722015876323, -0.17005424201488495, -1.459598422050476], [-2.2977514266967773, 0.44948890805244446, 0.5397163033485413]]\n",
    "    # 2,3 weights\n",
    "    for i in range(len(list(self.hidden_layer))):\n",
    "        #print(f\"hidden layer {i}\")\n",
    "        #print(\"weight of hidden layer\", self.hidden_layer[i].weight)\n",
    "        #print(\"calucalted: \", torch.tensor(self.hidden_layer[i].weight - (lr*x*d_hidden)) )\n",
    "        self.hidden_layer[i].weight = torch.nn.Parameter(self.hidden_layer[i].weight - (lr*x*d_hidden))\n",
    "            \n",
    "    # wjk = delta k * output of j.\n",
    "    # torch.stack(list(self.output_layer[0]))\n",
    "     # # [[0., 0., 0.]] # 1,1,3\n",
    "    #for output_index in range(len(list(model.output_layer))):\n",
    "        \n",
    "        \n",
    "    # print(f\"d_outputs\", d_outputs)\n",
    "    # print(\"output layer parameters\", self.output_layer[0].weight)  \n",
    "    return\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
